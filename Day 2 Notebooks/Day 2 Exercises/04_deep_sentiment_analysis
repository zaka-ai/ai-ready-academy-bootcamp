{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_deep_sentiment_analysis","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"IT3_7pSqQtmQ"},"source":["#Deep Sentiment Analaysis\n","\n","**Objective:** The goal from this exercise is to learn how to integrate Deep Learning into Natural Language Processing through Deep Sentiment Analysis.\n","The sections of this colab exercise are:\n","1. Keras Embedding Layer\n","2. Dataset loading\n","3. Data preparation\n","4. Feature extraction using Word Embeddings\n","5. Recurrent Neural Network model\n","6. Plotting training details"]},{"cell_type":"markdown","metadata":{"id":"QLzGR6lhUF9g"},"source":["# Keras Embedding Layer\n","\n","Before we start with the Sentiment Analysis exercise, let's look at an example of how to use a Keras Embedding layer. \n","\n","In this example, we will build a `Sequential` model with an `Embedding` layer to learn the embeddings of a series of simple documents defined in the docs variable.\n"]},{"cell_type":"code","metadata":{"id":"MRpZ3TlXUBsZ"},"source":["from numpy import array\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten\n","from tensorflow.keras.layers import Embedding\n","\n","# define documents\n","docs = ['Well done!',\n","\t\t'Good work',\n","\t\t'Great effort',\n","\t\t'nice work',\n","\t\t'Excellent!',\n","\t\t'Weak',\n","\t\t'Poor effort!',\n","\t\t'not good',\n","\t\t'poor work',\n","\t\t'Could have done better.']\n","\n","# define class labels\n","labels = array([1,1,1,1,1,0,0,0,0,0])\n","\n","# prepare tokenizer\n","t = Tokenizer()\n","t.fit_on_texts(docs)\n","vocab_size = len(t.word_index) + 1\n","\n","# integer encode the documents\n","encoded_docs = t.texts_to_sequences(docs)\n","print(encoded_docs)\n","\n","# pad documents to a max length of 4 words\n","max_length = 4\n","padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n","print(padded_docs)\n","\n","# define the model\n","model = Sequential()\n","model.add(Embedding(vocab_size, 8, input_length=max_length))\n","model.add(Flatten())\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n","\n","# summarize the model\n","model.summary()\n","\n","# fit the model\n","model.fit(padded_docs, labels, epochs=50, verbose=0)\n","\n","# evaluate the model\n","loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n","print('Accuracy: %f' % (accuracy*100))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Weql_nx1bKZI"},"source":["# Sentiment Analysis\n","\n","Text classification is one of the important tasks of text mining.\n","\n","![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n","\n","In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review. \n","\n","We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative).\n","\n","\n","\n","\n","---\n","\n","\n","This is the same exercise we saw in the previous lesson but the differences here are:\n","\n","\n","*   We are using **Word Embeddings** for feature extraction instead of Bag-of-Words. This is done by adding an `Embedding` layer as the first layer in the Sequential model.\n","*   We are using a deep **Recurrent Neural Network** for modeling.\n","\n","These changes should allow the model to better understand the dataset and give better results."]},{"cell_type":"markdown","metadata":{"id":"1NG9CLMaT0N7"},"source":["## Data Preparation"]},{"cell_type":"markdown","metadata":{"id":"K_HVs_6nS2F2"},"source":["### 1. Import Modules"]},{"cell_type":"code","metadata":{"id":"TxDGpou5cAzB"},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","# download Punkt Sentence Tokenizer\n","nltk.download('punkt')\n","# download stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5blEsxShTEV"},"source":["### 2. Download and Load Dataset"]},{"cell_type":"code","metadata":{"id":"m2b90HnObMkm"},"source":["# download IMDB dataset\n","!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"  \n","\n","# list files in current directory\n","!ls -lah  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9v-9xJvhbb9c"},"source":["# the path to the IMDB dataset\n","dataset_path = 'movie_data.csv'  \n","\n","# read file (dataset) into our program using pandas\n","data = pd.read_csv(dataset_path) \n","\n","# display first 5 rows\n","data.head()  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2sAgqwiZbzuU"},"source":["### 3. Clean Text\n","\n","Define the `clean_review` function to apply on the dataset reviews."]},{"cell_type":"code","metadata":{"id":"Eyrg00Ycb08M"},"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","\n","english_stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","def clean_review(text):\n","  # convert to lower case\n","  text = text.lower()\n","\n","  # remove none alphabetic characters\n","  text = re.sub(r'[^a-z]', ' ', text)\n","\n","  # stem words \n","  # split into words\n","  tokens = word_tokenize(text)\n","\n","  # stemming of words\n","  stemmed = [stemmer.stem(word) for word in tokens]\n","\n","  text = ' '.join(stemmed)\n","\n","  # remove stopwords\n","  text = ' '.join([word for word in text.split() if word not in english_stopwords])\n","\n","  return text\n","\n","\n","# apply to all dataset\n","data['clean_review'] = data['review'].apply(clean_review)\n","data.head() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7pSqLwbYcNqu"},"source":["### 4. Split Dataset"]},{"cell_type":"code","metadata":{"id":"3ZlGI1CScPLu"},"source":["from sklearn.model_selection import train_test_split\n"," \n","X = data['clean_review'].values\n","y = data['sentiment'].values\n","\n","# Split data into 50% training & 50% test\n","# let's all use a random state of 42 for example to ensure having the same split\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjYirO0ucXUw"},"source":["### 5. Feature Extraction with Word Embeddings\n","\n","Instead of going with Bag-of-Words for feature extraction, we are using Keras'  `Tokenizer()` class to prepare the data for the `Embedding` layer.\n"]},{"cell_type":"code","metadata":{"id":"TuS3-0_3cfUP"},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# define your tokenizer (with num_words=10000)\n","tokenizer_obj = _______\n","\n","# assign an index (number) to each word using fit_on_texts function  \n","tokenizer_obj._______\n","\n","# will be used later to pad sequences\n","max_length = 120\n","\n","# define vocabulary size\n","vocab_size = len(tokenizer_obj.word_index) + 1\n","\n","# transform each text to a sequence of integers (to be used later in embeddings layer)\n","X_train_tokens =  tokenizer_obj._______\n","X_test_tokens = _______\n","\n","\n","# apply post-padding to the sequences\n","X_train_pad = pad_sequences(X_train_tokens, maxlen=max_length, padding='post')\n","X_test_pad = pad_sequences(X_test_tokens, maxlen=max_length, padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJ67srOxdbu8"},"source":["x_train[0], X_train_pad[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yqg6oLbyeMzM"},"source":["## Recurrent Neural Network\n","\n","Now it's time to build the deep RNN network that will model the data. The network has to start with an `Embedding` layer, then we add one or multiple Recurrent layers and finally finish with a couple of Dense layers."]},{"cell_type":"markdown","metadata":{"id":"t8GdbHJ2T_QO"},"source":["### Building and Training the RNN model"]},{"cell_type":"code","metadata":{"id":"RqAkP-UodqOE"},"source":["from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","\n","embedding_dim = 300\n","\n","# FILL BLANKS\n","# build the neural network\n","\n","\n","\n","# compile model: assign loss & optimizer\n","model.compile(loss = 'binary_crossentropy',\n","              optimizer = 'adam',\n","              metrics=['accuracy'])\n","\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tlKvxbu8fpMK"},"source":["# train model\n","model.fit(X_train_pad, y_train, batch_size=32, epochs=5, validation_data=(X_test_pad, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkXHWbrCg0PD"},"source":["### Plot training details\n","\n","We visualize the training parameters to have a better understanding of the model's convergence."]},{"cell_type":"code","metadata":{"id":"BqZrCxAGfxp_"},"source":["def plot_accuracy_and_loss(model):\n","    epochs = model.history.params['epochs']\n","    epochs = range(epochs)\n","    val_loss = model.history.history['val_loss']\n","    val_accuracy = model.history.history['val_accuracy']\n","    training_loss = model.history.history['loss']\n","    training_accuracy = model.history.history['accuracy']\n","\n","    plt.plot(epochs, val_loss, 'r', label='test')\n","    plt.plot(epochs, training_loss, 'b', label='training')\n","    plt.xlabel('epochs')\n","    plt.ylabel('Loss')\n","    plt.legend(loc='upper right')\n","    plt.grid(True)\n","    plt.show()\n","\n","    plt.plot(epochs, val_accuracy, 'r', label='test')\n","    plt.plot(epochs, training_accuracy, 'b', label='training')\n","    plt.xlabel('epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend(loc='lower right')\n","    plt.grid(True)\n","    plt.show()\n","\n","plot_accuracy_and_loss(model)"],"execution_count":null,"outputs":[]}]}