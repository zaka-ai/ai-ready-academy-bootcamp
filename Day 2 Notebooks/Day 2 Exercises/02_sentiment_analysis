{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_sentiment_analysis","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Q24hSxA4tA6n"},"source":["# Text classification\n","\n","Text classification is one of the important tasks of text mining\n","\n","![alt text](http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1535125878/NLTK3_zwbdgg.png)\n","\n","In this notebook, we will perform Sentiment Analysis on IMDB movies reviews. Sentiment Analysis is the art of extracting people's opinion from digital text. We will use a regression model from Scikit-Learn able to predict the sentiment given a movie review. \n","\n","We will use [the IMDB movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/), which consists of 50,000 movies review (50% are positive, 50% are negative)."]},{"cell_type":"markdown","metadata":{"id":"0O1jA8byt4bV"},"source":["The libraries needed in this exercise are:\n","* [Numpy](http://www.numpy.org/) — a package for scientific computing.\n","* [Pandas](https://pandas.pydata.org/) — a library providing high-performance, easy-to-use data structures and data analysis tools for the Python\n","* [Matplotlib](https://matplotlib.org/) — a package for plotting & visualizations.\n","* [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n","* [NLTK](http://www.nltk.org/) — a platform to work with natural language."]},{"cell_type":"markdown","metadata":{"id":"844CS6rf57X7"},"source":["##Loading the data"]},{"cell_type":"markdown","metadata":{"id":"QAt6rj955meo"},"source":["### Importing the libraries and necessary dictionaries"]},{"cell_type":"code","metadata":{"id":"RRN4WqkltlB5"},"source":["import numpy as np\n","import pandas as pd\n","import nltk\n","import matplotlib.pyplot as plt\n","from tensorflow import keras\n","\n","# download Punkt Sentence Tokenizer\n","nltk.download('punkt')\n","# download stopwords\n","nltk.download('stopwords')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7duM74C95rhN"},"source":["### Loading the dataset in our directory"]},{"cell_type":"code","metadata":{"id":"c48UYWDcg3hR"},"source":["# download IMDB dataset\n","!wget \"https://raw.githubusercontent.com/javaidnabi31/Word-Embeddding-Sentiment-Classification/master/movie_data.csv\" -O \"movie_data.csv\"  \n","\n","# list files in current directory\n","!ls -lah  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"77spW4xt5y4R"},"source":["###Reading the dataset file and getting info on it\n","Using pandas to read the csv file and displaying the first 5 rows"]},{"cell_type":"code","metadata":{"id":"R0A5QhDlteWj"},"source":["# path to IMDB dataseet\n","\n","\n","# read file (dataset) into our program using pandas\n","\n","\n","# display first 5 rows\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t8oHmgm-6qK2"},"source":["Getting info on our dataset"]},{"cell_type":"code","metadata":{"id":"uQVx6AhqhAiB"},"source":["data.info()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cPbcG_8k54JZ"},"source":["A balanced dataset in sentiment analysis is a dataset which holds an equal amount of positive sentiment data and negative sentiment data, meaning 50% of the data is positive and 50% is negative"]},{"cell_type":"code","metadata":{"id":"q12nMYY5vPhn"},"source":["# check if dataset is balanced (number of positive sentiment = number of negative sentiment) \n","# by plotting the different classes\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R4uAuueIwKkS"},"source":["## Text cleaning"]},{"cell_type":"code","metadata":{"id":"qCxs0pSovUOa"},"source":["print(data.review[10])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAvczEBgxUWl"},"source":["Let's define a function that would clean each movie review (sentence)"]},{"cell_type":"code","metadata":{"id":"eKKIsHqZwRJR"},"source":["import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem.porter import PorterStemmer\n","\n","english_stopwords = stopwords.words('english')\n","stemmer = PorterStemmer()\n","\n","# define cleaning function\n","def clean_review(text):\n","  # convert to lower case\n","  \n","  \n","  # remove non alphabetic characters\n","  \n","  \n","  # stem words\n","  # tokenize sentences\n","  \n","\n","  # Porter Stemmer\n","  \n","\n","  # reconstruct the text\n","  \n","\n","  # remove stopwords\n","  \n","\n","  return text"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-NIqPBfK67Zc"},"source":["And try it out on an instance of the dataset"]},{"cell_type":"code","metadata":{"id":"W4Bn3r1wzvwR"},"source":["print(data['review'][1])\n","print(clean_review(data['review'][1]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"24Ycze9C6_yb"},"source":["And now clean the entire dataset reviews"]},{"cell_type":"code","metadata":{"id":"6kHxWkPTz5eA"},"source":["# apply to all dataset\n","data['clean_review'] = data['review'].apply(clean_review)\n","data.head() "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zkVqSSzu2Ax8"},"source":["## Split dataset for training and testing\n","We will split our data into two subsets: a 50% subset will be used for training the model for prediction and the remaining 50% will be used for evaluating or testing its performance. The random state ensures reproducibility of the results."]},{"cell_type":"code","metadata":{"id":"QPHlwVS71brN"},"source":["from sklearn.model_selection import train_test_split\n"," \n","X = data['clean_review'].values\n","y = data['sentiment'].values\n","\n","# Split data into 50% training & 50% test\n","# Use a random state of 42 for example to ensure having the same split\n","# BLANK\n","\n","\n","print(x_train.shape, y_train.shape)\n","print(x_test.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wz23g0nD2nhN"},"source":["## Feature extraction with Bag of Words\n","In this section, let's apply the Bag of Words method to learn the vocabulary of our text and with it transform our training input data."]},{"cell_type":"code","metadata":{"id":"0_B0vrn-2sON"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","# FILL BLANKS\n","# define a CountVectorizer (with binary=True and max_features=10000)\n","vectorizer = _______\n","\n","# learn the vocabulary of all tokens in our training dataset\n","vectorizer._______ \n","\n","# transform x_train to bag of words\n","x_train_bow = vectorizer._______\n","x_test_bow = _______\n","\n","print(x_train_bow.shape, y_train.shape)\n","print(x_test_bow.shape, y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UtLaJfuw4060"},"source":["## Classification\n","Our data is ready for classification. Let's use [LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"]},{"cell_type":"code","metadata":{"id":"9mS51YGO4hfv"},"source":["from sklearn.linear_model import LogisticRegression\n","\n","# define the LogisticRegression classifier\n","\n","\n","# train the classifier on the training data\n","\n","\n","# get the mean accuracy on the training data\n","\n","\n","print('Training Accuracy:', acc_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Csw7GEm76E5"},"source":["Evaluating the performance of our model through its accuracy score"]},{"cell_type":"code","metadata":{"id":"sBJnyoqO5NyE"},"source":["# Evaluate model with test data\n","acc_test = model.score(x_test_bow, y_test)  \n","print('Test Accuracy:', acc_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yh5927-d6Gq4"},"source":["## Let's use the model to predict!\n","To do so, let's create a predict function which takes as argument our model and the bag of words vectorizer together with a review on which it would predict the sentiment. This review should be cleaned with the `clean_review` function we built, transformed by bag of words and then used for prediction with `model.predict()`."]},{"cell_type":"code","metadata":{"id":"u6kxkZ5m55Ii"},"source":["# define predict function\n","def predict(model, vectorizer, review):\n","    review = clean_review(review)\n","    review_bow = vectorizer.transform([review])\n","    return model.predict(review_bow)[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VrNunL18l4a"},"source":["And let's try it out on an example"]},{"cell_type":"code","metadata":{"id":"8z6WCl916flD"},"source":["review = 'The movie was great!'\n","predict(model, vectorizer, review)"],"execution_count":null,"outputs":[]}]}